# -*- coding: utf-8 -*-
"""AIM-model training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wA4pzXJRY6N_LXJZLa33CuTRQPCYlkGc
"""
from numpy import reshape  # for reshaping input
from glob import glob  # for selecting midi files
from pickle import dump  # for dumping total notes
from keras.utils.np_utils import to_categorical  # for converting class vector to binary class matrix
from music21 import converter, note, chord  # for parsing MIDI files
from matplotlib.pyplot import xlabel, ylabel, title, plot, show  # for plotting the loss graph
from keras.models import Sequential  # our model class
from keras.layers import LSTM, Dense, Dropout  # for adding layers to our model
from keras.callbacks import ModelCheckpoint  # for keeping the latest trained model

total_notes = []
genre = input()

for midi in glob(f'midi_files/{genre}/*.mid'):
    parsed = converter.parse(midi)
    flat_elements = parsed.flat.notes
    for e in flat_elements:
        if isinstance(e, note.Note):
            total_notes.append(str(e.pitch))
        elif isinstance(e, chord.Chord):
            total_notes.append("+".join(str(a) for a in e.normalOrder))

# dump total_notes
with open(f"{genre}_total_notes", 'wb') as filepath:
    dump(total_notes, filepath)
filepath.close()

unique_notes = len(set(total_notes))

len_seq = 100  # sequence length
pitches = sorted(set(total_notes))
elem_to_num = dict((e, n) for n, e in enumerate(pitches))

# network input and output
_input = []
output = []

for x in range(len(total_notes) - len_seq):
    input_seq = total_notes[x:x + len_seq]
    output_seq = total_notes[x + len_seq]
    _input.append([elem_to_num[char] for char in input_seq])
    output.append(elem_to_num[output_seq])

patterns = len(_input)  # number of examples

# reshaping and normalising input
_input = reshape(_input, (patterns, len_seq, 1))
input_normalized = _input / float(unique_notes)

output = to_categorical(output)

# creating model and adding layers
model = Sequential()
model.add(LSTM(units=512,
               input_shape=(input_normalized.shape[1], input_normalized.shape[2]),
               return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(512, return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(512))
model.add(Dropout(0.3))
model.add(Dense(256))
model.add(Dropout(0.3))
model.add(Dense(unique_notes, activation="softmax"))
model.compile(loss="categorical_crossentropy", optimizer="adam")

# save model
checkpoint = ModelCheckpoint(f"{genre}_model.hdf5", monitor='loss', verbose=0, save_best_only=True, mode='min')
model_history = model.fit(input_normalized, output, epochs=100, batch_size=128, callbacks=[checkpoint])

epochs = range(1, 101)

# plot the loss graph
plot(epochs, model_history.history['loss'])
title('Training loss')
xlabel('Epochs')
ylabel('Loss')
show()
